"""
Export du modÃ¨le au format TensorFlow Lite pour dÃ©ploiement mobile
"""
import os
import numpy as np
import tensorflow as tf
from tensorflow import keras
from ai_edge_litert.interpreter import Interpreter
import config


def extract_keypoints_from_heatmaps(heatmaps, frame_shape):
    """Extrait les coordonnÃ©es des keypoints depuis les heatmaps"""
    h, w = frame_shape[:2]
    keypoints = []

    for i in range(heatmaps.shape[-1]):
        heatmap = heatmaps[:, :, i]
        max_pos = np.unravel_index(heatmap.argmax(), heatmap.shape)
        y = int(max_pos[0] * h / heatmap.shape[0])
        x = int(max_pos[1] * w / heatmap.shape[1])
        confidence = heatmap[max_pos]
        keypoints.append({'x': x, 'y': y, 'confidence': confidence})

    return keypoints

def convert_to_tflite(model, output_path, quantize=True, quantization_type='int8', representative_dataset=None):
    """
    Convertit un modÃ¨le Keras en TensorFlow Lite
    
    Args:
        model_path: Chemin vers le modÃ¨le SavedModel ou .h5
        output_path: Chemin de sortie pour le fichier .tflite
        quantize: Activer la quantization
        quantization_type: Type de quantization ('int8', 'float16', 'dynamic', 'none')
        representative_dataset: Dataset reprÃ©sentatif pour la quantization
    
    Returns:
        tflite_model_size: Taille du modÃ¨le en Ko
    """
    converter = tf.lite.TFLiteConverter.from_keras_model(model)
    
    # Configuration du converter selon le type de quantization
    if quantize:
        if quantization_type == 'int8':
            print("\nConfiguration de la quantization INT8 optimisÃ©e...")
            converter.optimizations = [tf.lite.Optimize.DEFAULT]
            converter.inference_input_type = tf.uint8
            converter.inference_output_type = tf.uint8
            converter.target_spec.supported_ops = [
                tf.lite.OpsSet.TFLITE_BUILTINS_INT8,
                tf.lite.OpsSet.TFLITE_BUILTINS,
            ]
            if representative_dataset is not None:
                converter.representative_dataset = representative_dataset
                
        elif quantization_type == 'float16':
            print("\nConfiguration de la quantization FLOAT16 (haute prÃ©cision)...")
            converter.optimizations = [tf.lite.Optimize.DEFAULT]
            converter.target_spec.supported_types = [tf.float16]
            
        elif quantization_type == 'dynamic':
            print("\nConfiguration de la quantization dynamique (range-based)...")
            converter.optimizations = [tf.lite.Optimize.DEFAULT]
            if representative_dataset is not None:
                converter.representative_dataset = representative_dataset
            # Les poids sont quantizÃ©s dynamiquement, entrÃ©es/sorties restent float32
            
        else:
            raise ValueError(f"Type de quantization non supportÃ©: {quantization_type}")
    else:
        print("\nâš™ï¸  Pas de quantization (modÃ¨le float32 complet)")
    
    # Convertir
    print("\nConversion en cours...")
    tflite_model = converter.convert()
    
    # Sauvegarder
    with open(output_path, 'wb') as f:
        f.write(tflite_model)
    
    # Afficher la taille
    tflite_model_size = len(tflite_model) / 1024  # en Ko
    print(f"\nâœ… ModÃ¨le TFLite sauvegardÃ©: {output_path}")
    print(f"ğŸ“Š Taille du modÃ¨le: {tflite_model_size:.2f} Ko")
    print(f"ğŸ¯ Type de quantization: {quantization_type.upper()}")
    
    print("=" * 60)
    
    return tflite_model_size


def create_representative_dataset_generator(X_val, num_samples=100):
    """
    CrÃ©e un gÃ©nÃ©rateur de dataset reprÃ©sentatif pour la quantization
    AMÃ‰LIORÃ‰: Utilise plus d'Ã©chantillons et couvre mieux la distribution
    
    Args:
        X_val: Dataset de validation
        num_samples: Nombre d'Ã©chantillons Ã  utiliser (augmentÃ© pour meilleure calibration)
    
    Returns:
        representative_dataset_gen: GÃ©nÃ©rateur pour le converter
    """
    def representative_dataset_gen():
        # AMÃ‰LIORATION 2: Utiliser TOUS les Ã©chantillons de validation pour meilleure calibration
        # Au lieu de prendre sÃ©quentiellement, on mÃ©lange pour couvrir toute la distribution
        indices = np.random.permutation(len(X_val))[:num_samples]
        for idx in indices:
            # Prendre un Ã©chantillon
            sample = X_val[idx:idx+1].astype(np.float32)
            yield [sample]
    
    return representative_dataset_gen


def test_tflite_model(tflite_path, val_ds,  num_samples=10):
    """
    Teste le modÃ¨le TFLite et compare avec les prÃ©dictions originales
    
    Args:
        tflite_path: Chemin vers le modÃ¨le .tflite
        X_test: Images de test
        y_test: Heatmaps de test
        num_samples: Nombre d'Ã©chantillons Ã  tester
    
    Returns:
        avg_error: Erreur moyenne
    """
    print("\nğŸ§ª Test du modÃ¨le TFLite...")
    
    # Charger l'interprÃ©teur TFLite
    interpreter = Interpreter(model_path=tflite_path)
    interpreter.allocate_tensors()
    
    # Obtenir les dÃ©tails des entrÃ©es/sorties
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()
    
    print(f"\nğŸ“Š DÃ©tails de l'interprÃ©teur:")
    print(f"   - Input shape: {input_details[0]['shape']}")
    print(f"   - Input type: {input_details[0]['dtype']}")
    print(f"   - Output shape: {output_details[0]['shape']}")
    print(f"   - Output type: {output_details[0]['dtype']}")
    
    # Tester sur quelques Ã©chantillons
    # get images and labels from test set
    all_images = []
    all_labels = []
    for images, labels in val_ds.unbatch().take(num_samples):
        # images = tf.cast(images, tf.uint8)
        images = np.array(images)[None, ...]
        labels = (np.array(labels) * 255).astype(np.uint8)[None, ...] 
        # convert all colors of heatmaps to red
        # labels = np.stack([labels, labels, labels], axis=-1)
        all_images.append(images)
        all_labels.append(labels)
    
    errors = []
    for i in range(min(num_samples, len(all_images))):
        # PrÃ©parer l'entrÃ©e
        input_data = all_images[i].astype(np.float32)
        input_data = tf.cast(input_data, tf.float32)
        # normalize between -1 and 1
        # input_data = (input_data / 127.5) - 1
        
        # Si le modÃ¨le attend des uint8, il faut quantizer l'entrÃ©e
        if input_details[0]['dtype'] == np.uint8:
            input_scale, input_zero_point = input_details[0]['quantization']
            input_data = (input_data / input_scale + input_zero_point)
            input_data = tf.cast(input_data, tf.uint8)
        
        # InfÃ©rence
        interpreter.set_tensor(input_details[0]['index'], input_data)
        interpreter.invoke()
        
        # RÃ©cupÃ©rer la sortie
        output_data = interpreter.get_tensor(output_details[0]['index'])
        
        # Si la sortie est quantizÃ©e, il faut la dÃ©quantizer
        if output_details[0]['dtype'] == np.uint8:
            output_scale, output_zero_point = output_details[0]['quantization']
            output_data = (output_data.astype(np.float32) - output_zero_point) * output_scale

        # Calculer l'erreur
        # error = np.mean(np.abs(output_data - all_labels[i]))
        # # error in pixel 
        # all_labels_pixel = all_labels[i] * input_data.shape[1]
        # output_data_pixel = output_data * input_data.shape[1]
        # error_pixel = np.mean(np.abs(output_data_pixel - all_labels_pixel))
        # errors.append(error * 100)
    
    
    # return avg_error


def export_model(model, model_name="pose_model", model_dir=None, representative_ds=None):
    """
    Pipeline complet d'export du modÃ¨le en TFLite avec deux versions optimisÃ©es
    
    Args:
        model: ModÃ¨le Keras (optionnel si model_path est fourni)
        model_path: Chemin vers le modÃ¨le sauvegardÃ© (optionnel si model est fourni)
        X_val: Dataset de validation pour la quantization
        model_name: Nom du modÃ¨le
        model_dir: Dossier racine du modÃ¨le (si None, utilise config.MODELS_DIR)
    
    Returns:
        tflite_paths: Dictionnaire avec les chemins des modÃ¨les exportÃ©s
    """
    
    # DÃ©terminer le dossier des modÃ¨les
    models_dir = config.MODELS_DIR if model_dir is None else os.path.join(model_dir, "models")
    
    tflite_paths = {}
    
    # CrÃ©er le dataset reprÃ©sentatif si nÃ©cessaire
    representative_dataset = None
    if representative_ds is not None:
        def representative_dataset_gen():
            for images, _ in representative_ds.unbatch().take(100):
                images = tf.cast(images, tf.float32)
                yield [tf.expand_dims(images, 0)]

    print("\n" + "=" * 40)
    print("ğŸ“± EXPORT 1/2 - DYNAMIC RANGE QUANTIZATION")
    print("=" * 40)
    print("ğŸ¯ RECOMMANDÃ‰: PrÃ©cision optimale + taille rÃ©duite")
    
    tflite_dynamic_path = os.path.join(models_dir, f"{model_name}_dynamic.tflite")
    dynamic_size = convert_to_tflite(
        model=model,
        output_path=tflite_dynamic_path,
        quantize=True,
        quantization_type='dynamic',
        representative_dataset=None  # Dynamic n'a pas besoin de dataset reprÃ©sentatif
    )
    tflite_paths['dynamic'] = tflite_dynamic_path
    
    # Export 2: ModÃ¨le Float32 complet (haute prÃ©cision)
    print("\n" + "=" * 40)
    print("ğŸ”¬ EXPORT 2/2 - FLOAT32 COMPLET")
    print("=" * 40)
    print("ğŸ¯ TESTS: PrÃ©cision maximale (taille importante)")
    
    tflite_float32_path = os.path.join(models_dir, f"{model_name}_float32.tflite")
    float32_size = convert_to_tflite(
        model=model,
        output_path=tflite_float32_path,
        quantize=False,
        quantization_type='none',
        representative_dataset=None
    )
    tflite_paths['float32'] = tflite_float32_path

    # Export 3: ModÃ¨le int8 (smalest)
    print("\n" + "=" * 40)
    print("ğŸ”¬ EXPORT 2/2 - FLOAT32 COMPLET")
    print("=" * 40)
    print("ğŸ¯ TESTS: PrÃ©cision maximale (taille importante)")
    
    tflite_int8_path = os.path.join(models_dir, f"{model_name}_int8.tflite")
    int8_size = convert_to_tflite(
        model=model,
        output_path=tflite_int8_path,
        quantize=True,
        quantization_type='int8',
        representative_dataset=representative_dataset_gen
    )
    tflite_paths['int8'] = tflite_int8_path
    
    print(f"\nâœ… Exports terminÃ©s!")
    print(f"ğŸ“± ModÃ¨le Dynamic: {tflite_dynamic_path} ({dynamic_size:.1f} Ko)")
    print(f"ğŸ”¬ ModÃ¨le Float32: {tflite_float32_path} ({float32_size:.1f} Ko)")
    
    # Comparaison des modÃ¨les
    print("\n" + "=" * 60)
    print("ï¿½ COMPARAISON DES MODÃˆLES EXPORTÃ‰S")
    print("=" * 60)
    print("ModÃ¨le         | Taille | PrÃ©cision | Usage recommandÃ©")
    print("-" * 60)
    print(f"Dynamic (.tflite) | {dynamic_size:>5.1f} Ko | ~1px erreur | PRODUCTION MOBILE â­")
    print(f"Float32 (.tflite) | {float32_size:>5.1f} Ko | ~0px erreur | TESTS/VALIDATION")
    print("=" * 60)
    
    # Instructions pour l'utilisation
    print("\nğŸ“± UTILISATION DANS FLUTTER")
    print("=" * 60)
    print("ğŸ¤– Pour production mobile:")
    print(f"   ğŸ“ Utilisez: {os.path.basename(tflite_dynamic_path)}")
    print("   âœ… PrÃ©cision suffisante + taille optimisÃ©e")
    print("   ğŸš€ Compatible avec GPU/NNAPI delegates")
    
    print("\nğŸ”¬ Pour tests/validation:")
    print(f"   ğŸ“ Utilisez: {os.path.basename(tflite_float32_path)}")
    print("   âœ… PrÃ©cision maximale")
    print("   ğŸŒ Plus lent, taille importante")
    
    print("\nğŸ“‹ ParamÃ¨tres communs:")
    print("   â€¢ Input: 192Ã—192Ã—3 float32 (0-1 normalisÃ©)")
    print("   â€¢ Output: 48Ã—48Ã—3 float32 (heatmaps)")
    print("   â€¢ Keypoints: [0]=Hanche, [1]=Genou, [2]=Cheville")
    print("=" * 60)
    
    return tflite_paths
    
    # Si un modÃ¨le Keras est fourni, le sauvegarder d'abord
    if model is not None:
        saved_model_dir = os.path.join(config.MODELS_DIR, f"{model_name}_for_export")
        print(f"\nğŸ’¾ Sauvegarde du modÃ¨le au format SavedModel...")
        model.save(saved_model_dir, save_format='tf')
        model_path = saved_model_dir
    
    if model_path is None:
        raise ValueError("Vous devez fournir soit 'model' soit 'model_path'")
    
    # Chemin de sortie pour le .tflite
    tflite_filename = f"{model_name}_{quantization_type}.tflite"
    tflite_path = os.path.join(config.MODELS_DIR, tflite_filename)
    
    # CrÃ©er le dataset reprÃ©sentatif si nÃ©cessaire
    representative_dataset = None
    if quantization_type == 'int8' and X_val is not None:
        num_calibration_samples = min(500, len(X_val))
        print(f"\nğŸ“Š CrÃ©ation du dataset reprÃ©sentatif ({num_calibration_samples} Ã©chantillons)...")
        representative_dataset = create_representative_dataset_generator(
            X_val, 
            num_samples=num_calibration_samples
        )
    
    # Convertir en TFLite
    quantize = quantization_type != 'none'
    tflite_size = convert_to_tflite(
        model_path=model_path,
        output_path=tflite_path,
        quantize=quantize,
        quantization_type=quantization_type,
        representative_dataset=representative_dataset
    )
    
    print(f"\nâœ… Export terminÃ©!")
    print(f"ğŸ“± ModÃ¨le prÃªt pour le dÃ©ploiement mobile: {tflite_path}")
    
    # Comparaison des tailles et prÃ©cisions
    print("\n" + "=" * 60)
    print("ğŸ“Š COMPARAISON DES OPTIONS DE QUANTIZATION")
    print("=" * 60)
    print("ğŸ¯ PrÃ©cision (dÃ©croissante) | Taille | Vitesse | Recommandation")
    print("-" * 60)
    print("âŒ Aucune (float32)       | ~25MB  | TrÃ¨s lent | DÃ©veloppement seulement")
    print("ğŸŸ¡ Float16                | ~12MB  | Moyen     | BON COMPROMIS â­")
    print("ğŸŸ  Dynamic Range          | ~6MB   | Rapide    | Mobile standard")
    print("ğŸ”´ INT8                   | ~6MB   | TrÃ¨s rapide | Production intensive")
    print("=" * 60)
    
    # Instructions pour l'utilisation
    print("\nğŸ“± UTILISATION DU MODÃˆLE TFLITE")
    print("=" * 60)
    print(f"\nğŸ”§ Type de quantization utilisÃ©: {quantization_type.upper()}")
    
    if quantization_type == 'float16':
        print("ğŸ’¡ RECOMMANDÃ‰ pour votre cas - PrÃ©cision proche du Keras avec bonne performance")
    elif quantization_type == 'none':
        print("âš ï¸  ATTENTION - ModÃ¨le trÃ¨s volumineux, utilisez seulement pour tests")
    
    print("\nğŸ¤– Android (Java/Kotlin):")
    print("   1. Ajoutez le .tflite dans assets/")
    print("   2. Ajoutez la dÃ©pendance: implementation 'org.tensorflow:tensorflow-lite:2.x.x'")
    print("   3. Chargez avec: Interpreter.create(...)")
    print("   4. Utilisez GPU Delegate ou NNAPI pour accÃ©lÃ©rer")
    
    print("\nğŸ iOS (Swift/Objective-C):")
    print("   1. Ajoutez le .tflite au projet Xcode")
    print("   2. Ajoutez TensorFlowLiteSwift via CocoaPods/SPM")
    print("   3. Chargez avec: Interpreter(modelPath: ...)")
    print("   4. Utilisez Metal Delegate pour accÃ©lÃ©rer")
    
    print("=" * 60)
    
    return tflite_path


if __name__ == "__main__":
    print("âœ… Module export_tflite.py chargÃ© avec succÃ¨s")
    print("ğŸ“ Utilisez main.py pour exporter le modÃ¨le aprÃ¨s l'entraÃ®nement")
