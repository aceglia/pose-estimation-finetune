"""
Export du modÃ¨le au format TensorFlow Lite pour dÃ©ploiement mobile
"""
import os
import numpy as np
import tensorflow as tf
from tensorflow import keras
import config


def convert_to_tflite(model_path, output_path, quantize=True, quantization_type='int8', representative_dataset=None):
    """
    Convertit un modÃ¨le Keras en TensorFlow Lite
    
    Args:
        model_path: Chemin vers le modÃ¨le SavedModel ou .h5
        output_path: Chemin de sortie pour le fichier .tflite
        quantize: Activer la quantization
        quantization_type: Type de quantization ('int8', 'float16', 'dynamic', 'none')
        representative_dataset: Dataset reprÃ©sentatif pour la quantization
    
    Returns:
        tflite_model_size: Taille du modÃ¨le en Ko
    """
    print("=" * 60)
    print("ğŸ“¦ CONVERSION EN TENSORFLOW LITE")
    print("=" * 60)
    
    # Charger le modÃ¨le
    print(f"\nğŸ“‚ Chargement du modÃ¨le depuis: {model_path}")
    
    # CrÃ©er le converter
    if model_path.endswith('.h5'):
        model = keras.models.load_model(model_path)
        converter = tf.lite.TFLiteConverter.from_keras_model(model)
    else:
        # SavedModel format
        converter = tf.lite.TFLiteConverter.from_saved_model(model_path)
    
    # Configuration du converter selon le type de quantization
    if quantize:
        if quantization_type == 'int8':
            print("\nâš™ï¸  Configuration de la quantization INT8 optimisÃ©e...")
            converter.optimizations = [tf.lite.Optimize.DEFAULT]
            converter.target_spec.supported_ops = [
                tf.lite.OpsSet.TFLITE_BUILTINS_INT8,
                tf.lite.OpsSet.TFLITE_BUILTINS
            ]
            if representative_dataset is not None:
                converter.representative_dataset = representative_dataset
                
        elif quantization_type == 'float16':
            print("\nâš™ï¸  Configuration de la quantization FLOAT16 (haute prÃ©cision)...")
            converter.optimizations = [tf.lite.Optimize.DEFAULT]
            converter.target_spec.supported_types = [tf.float16]
            
        elif quantization_type == 'dynamic':
            print("\nâš™ï¸  Configuration de la quantization dynamique (range-based)...")
            converter.optimizations = [tf.lite.Optimize.DEFAULT]
            # Les poids sont quantizÃ©s dynamiquement, entrÃ©es/sorties restent float32
            
        else:
            raise ValueError(f"Type de quantization non supportÃ©: {quantization_type}")
    else:
        print("\nâš™ï¸  Pas de quantization (modÃ¨le float32 complet)")
    
    # Convertir
    print("\nğŸ”„ Conversion en cours...")
    tflite_model = converter.convert()
    
    # Sauvegarder
    with open(output_path, 'wb') as f:
        f.write(tflite_model)
    
    # Afficher la taille
    tflite_model_size = len(tflite_model) / 1024  # en Ko
    print(f"\nâœ… ModÃ¨le TFLite sauvegardÃ©: {output_path}")
    print(f"ğŸ“Š Taille du modÃ¨le: {tflite_model_size:.2f} Ko")
    print(f"ğŸ¯ Type de quantization: {quantization_type.upper()}")
    
    print("=" * 60)
    
    return tflite_model_size


def create_representative_dataset_generator(X_val, num_samples=100):
    """
    CrÃ©e un gÃ©nÃ©rateur de dataset reprÃ©sentatif pour la quantization
    AMÃ‰LIORÃ‰: Utilise plus d'Ã©chantillons et couvre mieux la distribution
    
    Args:
        X_val: Dataset de validation
        num_samples: Nombre d'Ã©chantillons Ã  utiliser (augmentÃ© pour meilleure calibration)
    
    Returns:
        representative_dataset_gen: GÃ©nÃ©rateur pour le converter
    """
    def representative_dataset_gen():
        # AMÃ‰LIORATION 2: Utiliser TOUS les Ã©chantillons de validation pour meilleure calibration
        # Au lieu de prendre sÃ©quentiellement, on mÃ©lange pour couvrir toute la distribution
        indices = np.random.permutation(len(X_val))[:num_samples]
        for idx in indices:
            # Prendre un Ã©chantillon
            sample = X_val[idx:idx+1].astype(np.float32)
            yield [sample]
    
    return representative_dataset_gen


def test_tflite_model(tflite_path, X_test, y_test, num_samples=10):
    """
    Teste le modÃ¨le TFLite et compare avec les prÃ©dictions originales
    
    Args:
        tflite_path: Chemin vers le modÃ¨le .tflite
        X_test: Images de test
        y_test: Heatmaps de test
        num_samples: Nombre d'Ã©chantillons Ã  tester
    
    Returns:
        avg_error: Erreur moyenne
    """
    print("\nğŸ§ª Test du modÃ¨le TFLite...")
    
    # Charger l'interprÃ©teur TFLite
    interpreter = tf.lite.Interpreter(model_path=tflite_path)
    interpreter.allocate_tensors()
    
    # Obtenir les dÃ©tails des entrÃ©es/sorties
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()
    
    print(f"\nğŸ“Š DÃ©tails de l'interprÃ©teur:")
    print(f"   - Input shape: {input_details[0]['shape']}")
    print(f"   - Input type: {input_details[0]['dtype']}")
    print(f"   - Output shape: {output_details[0]['shape']}")
    print(f"   - Output type: {output_details[0]['dtype']}")
    
    # Tester sur quelques Ã©chantillons
    errors = []
    for i in range(min(num_samples, len(X_test))):
        # PrÃ©parer l'entrÃ©e
        input_data = X_test[i:i+1].astype(np.float32)
        
        # Si le modÃ¨le attend des uint8, il faut quantizer l'entrÃ©e
        if input_details[0]['dtype'] == np.uint8:
            input_scale, input_zero_point = input_details[0]['quantization']
            input_data = (input_data / input_scale + input_zero_point).astype(np.uint8)
        
        # InfÃ©rence
        interpreter.set_tensor(input_details[0]['index'], input_data)
        interpreter.invoke()
        
        # RÃ©cupÃ©rer la sortie
        output_data = interpreter.get_tensor(output_details[0]['index'])
        
        # Si la sortie est quantizÃ©e, il faut la dÃ©quantizer
        if output_details[0]['dtype'] == np.uint8:
            output_scale, output_zero_point = output_details[0]['quantization']
            output_data = (output_data.astype(np.float32) - output_zero_point) * output_scale
        
        # Calculer l'erreur
        error = np.mean(np.abs(output_data - y_test[i:i+1]))
        errors.append(error)
    
    avg_error = np.mean(errors)
    print(f"\nğŸ“Š RÃ©sultats du test:")
    print(f"   - Nombre d'Ã©chantillons testÃ©s: {len(errors)}")
    print(f"   - Erreur moyenne (MAE): {avg_error:.6f}")
    
    return avg_error


def export_model(model=None, model_path=None, X_val=None, model_name="pose_model", model_dir=None):
    """
    Pipeline complet d'export du modÃ¨le en TFLite avec deux versions optimisÃ©es
    
    Args:
        model: ModÃ¨le Keras (optionnel si model_path est fourni)
        model_path: Chemin vers le modÃ¨le sauvegardÃ© (optionnel si model est fourni)
        X_val: Dataset de validation pour la quantization
        model_name: Nom du modÃ¨le
        model_dir: Dossier racine du modÃ¨le (si None, utilise config.MODELS_DIR)
    
    Returns:
        tflite_paths: Dictionnaire avec les chemins des modÃ¨les exportÃ©s
    """
    print("=" * 60)
    print("ğŸš€ EXPORT DU MODÃˆLE EN TENSORFLOW LITE")
    print("=" * 60)
    
    # DÃ©terminer le dossier des modÃ¨les
    models_dir = config.MODELS_DIR if model_dir is None else os.path.join(model_dir, "models")
    
    # Si un modÃ¨le Keras est fourni, le sauvegarder d'abord
    if model is not None:
        saved_model_dir = os.path.join(models_dir, f"{model_name}_for_export")
        print(f"\nğŸ’¾ Sauvegarde du modÃ¨le au format SavedModel...")
        model.save(saved_model_dir, save_format='tf')
        model_path = saved_model_dir
    
    if model_path is None:
        raise ValueError("Vous devez fournir soit 'model' soit 'model_path'")
    
    tflite_paths = {}
    
    # CrÃ©er le dataset reprÃ©sentatif si nÃ©cessaire
    representative_dataset = None
    if X_val is not None:
        num_calibration_samples = min(500, len(X_val))
        print(f"\nğŸ“Š CrÃ©ation du dataset reprÃ©sentatif ({num_calibration_samples} Ã©chantillons)...")
        representative_dataset = create_representative_dataset_generator(
            X_val, 
            num_samples=num_calibration_samples
        )
    
    # Export 1: ModÃ¨le Dynamic Range Quantization (recommandÃ© pour mobile)
    print("\n" + "=" * 40)
    print("ğŸ“± EXPORT 1/2 - DYNAMIC RANGE QUANTIZATION")
    print("=" * 40)
    print("ğŸ¯ RECOMMANDÃ‰: PrÃ©cision optimale + taille rÃ©duite")
    
    tflite_dynamic_path = os.path.join(models_dir, f"{model_name}_dynamic.tflite")
    dynamic_size = convert_to_tflite(
        model_path=model_path,
        output_path=tflite_dynamic_path,
        quantize=True,
        quantization_type='dynamic',
        representative_dataset=None  # Dynamic n'a pas besoin de dataset reprÃ©sentatif
    )
    tflite_paths['dynamic'] = tflite_dynamic_path
    
    # Export 2: ModÃ¨le Float32 complet (haute prÃ©cision)
    print("\n" + "=" * 40)
    print("ğŸ”¬ EXPORT 2/2 - FLOAT32 COMPLET")
    print("=" * 40)
    print("ğŸ¯ TESTS: PrÃ©cision maximale (taille importante)")
    
    tflite_float32_path = os.path.join(models_dir, f"{model_name}_float32.tflite")
    float32_size = convert_to_tflite(
        model_path=model_path,
        output_path=tflite_float32_path,
        quantize=False,
        quantization_type='none',
        representative_dataset=None
    )
    tflite_paths['float32'] = tflite_float32_path
    
    print(f"\nâœ… Exports terminÃ©s!")
    print(f"ğŸ“± ModÃ¨le Dynamic: {tflite_dynamic_path} ({dynamic_size:.1f} Ko)")
    print(f"ğŸ”¬ ModÃ¨le Float32: {tflite_float32_path} ({float32_size:.1f} Ko)")
    
    # Comparaison des modÃ¨les
    print("\n" + "=" * 60)
    print("ï¿½ COMPARAISON DES MODÃˆLES EXPORTÃ‰S")
    print("=" * 60)
    print("ModÃ¨le         | Taille | PrÃ©cision | Usage recommandÃ©")
    print("-" * 60)
    print(f"Dynamic (.tflite) | {dynamic_size:>5.1f} Ko | ~1px erreur | PRODUCTION MOBILE â­")
    print(f"Float32 (.tflite) | {float32_size:>5.1f} Ko | ~0px erreur | TESTS/VALIDATION")
    print("=" * 60)
    
    # Instructions pour l'utilisation
    print("\nğŸ“± UTILISATION DANS FLUTTER")
    print("=" * 60)
    print("ğŸ¤– Pour production mobile:")
    print(f"   ğŸ“ Utilisez: {os.path.basename(tflite_dynamic_path)}")
    print("   âœ… PrÃ©cision suffisante + taille optimisÃ©e")
    print("   ğŸš€ Compatible avec GPU/NNAPI delegates")
    
    print("\nğŸ”¬ Pour tests/validation:")
    print(f"   ğŸ“ Utilisez: {os.path.basename(tflite_float32_path)}")
    print("   âœ… PrÃ©cision maximale")
    print("   ğŸŒ Plus lent, taille importante")
    
    print("\nğŸ“‹ ParamÃ¨tres communs:")
    print("   â€¢ Input: 192Ã—192Ã—3 float32 (0-1 normalisÃ©)")
    print("   â€¢ Output: 48Ã—48Ã—3 float32 (heatmaps)")
    print("   â€¢ Keypoints: [0]=Hanche, [1]=Genou, [2]=Cheville")
    print("=" * 60)
    
    return tflite_paths
    
    # Si un modÃ¨le Keras est fourni, le sauvegarder d'abord
    if model is not None:
        saved_model_dir = os.path.join(config.MODELS_DIR, f"{model_name}_for_export")
        print(f"\nğŸ’¾ Sauvegarde du modÃ¨le au format SavedModel...")
        model.save(saved_model_dir, save_format='tf')
        model_path = saved_model_dir
    
    if model_path is None:
        raise ValueError("Vous devez fournir soit 'model' soit 'model_path'")
    
    # Chemin de sortie pour le .tflite
    tflite_filename = f"{model_name}_{quantization_type}.tflite"
    tflite_path = os.path.join(config.MODELS_DIR, tflite_filename)
    
    # CrÃ©er le dataset reprÃ©sentatif si nÃ©cessaire
    representative_dataset = None
    if quantization_type == 'int8' and X_val is not None:
        num_calibration_samples = min(500, len(X_val))
        print(f"\nğŸ“Š CrÃ©ation du dataset reprÃ©sentatif ({num_calibration_samples} Ã©chantillons)...")
        representative_dataset = create_representative_dataset_generator(
            X_val, 
            num_samples=num_calibration_samples
        )
    
    # Convertir en TFLite
    quantize = quantization_type != 'none'
    tflite_size = convert_to_tflite(
        model_path=model_path,
        output_path=tflite_path,
        quantize=quantize,
        quantization_type=quantization_type,
        representative_dataset=representative_dataset
    )
    
    print(f"\nâœ… Export terminÃ©!")
    print(f"ğŸ“± ModÃ¨le prÃªt pour le dÃ©ploiement mobile: {tflite_path}")
    
    # Comparaison des tailles et prÃ©cisions
    print("\n" + "=" * 60)
    print("ğŸ“Š COMPARAISON DES OPTIONS DE QUANTIZATION")
    print("=" * 60)
    print("ğŸ¯ PrÃ©cision (dÃ©croissante) | Taille | Vitesse | Recommandation")
    print("-" * 60)
    print("âŒ Aucune (float32)       | ~25MB  | TrÃ¨s lent | DÃ©veloppement seulement")
    print("ğŸŸ¡ Float16                | ~12MB  | Moyen     | BON COMPROMIS â­")
    print("ğŸŸ  Dynamic Range          | ~6MB   | Rapide    | Mobile standard")
    print("ğŸ”´ INT8                   | ~6MB   | TrÃ¨s rapide | Production intensive")
    print("=" * 60)
    
    # Instructions pour l'utilisation
    print("\nğŸ“± UTILISATION DU MODÃˆLE TFLITE")
    print("=" * 60)
    print(f"\nğŸ”§ Type de quantization utilisÃ©: {quantization_type.upper()}")
    
    if quantization_type == 'float16':
        print("ğŸ’¡ RECOMMANDÃ‰ pour votre cas - PrÃ©cision proche du Keras avec bonne performance")
    elif quantization_type == 'none':
        print("âš ï¸  ATTENTION - ModÃ¨le trÃ¨s volumineux, utilisez seulement pour tests")
    
    print("\nğŸ¤– Android (Java/Kotlin):")
    print("   1. Ajoutez le .tflite dans assets/")
    print("   2. Ajoutez la dÃ©pendance: implementation 'org.tensorflow:tensorflow-lite:2.x.x'")
    print("   3. Chargez avec: Interpreter.create(...)")
    print("   4. Utilisez GPU Delegate ou NNAPI pour accÃ©lÃ©rer")
    
    print("\nğŸ iOS (Swift/Objective-C):")
    print("   1. Ajoutez le .tflite au projet Xcode")
    print("   2. Ajoutez TensorFlowLiteSwift via CocoaPods/SPM")
    print("   3. Chargez avec: Interpreter(modelPath: ...)")
    print("   4. Utilisez Metal Delegate pour accÃ©lÃ©rer")
    
    print("=" * 60)
    
    return tflite_path


if __name__ == "__main__":
    print("âœ… Module export_tflite.py chargÃ© avec succÃ¨s")
    print("ğŸ“ Utilisez main.py pour exporter le modÃ¨le aprÃ¨s l'entraÃ®nement")
