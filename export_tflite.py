"""
Export du modÃ¨le au format TensorFlow Lite pour dÃ©ploiement mobile
"""
import os
import numpy as np
import tensorflow as tf
from tensorflow import keras
from ai_edge_litert.interpreter import Interpreter
import config
from validation_utils import from_heatmaps_to_coords
import time
from train_utils import HeatmapToCoordinates

def extract_keypoints_from_heatmaps(heatmaps, frame_shape):
    """Extrait les coordonnÃ©es des keypoints depuis les heatmaps"""
    h, w = frame_shape[:2]
    keypoints = []

    for i in range(heatmaps.shape[-1]):
        heatmap = heatmaps[:, :, i]
        max_pos = np.unravel_index(heatmap.argmax(), heatmap.shape)
        y = int(max_pos[0] * h / heatmap.shape[0])
        x = int(max_pos[1] * w / heatmap.shape[1])
        confidence = heatmap[max_pos]
        keypoints.append({'x': x, 'y': y, 'confidence': confidence})

    return keypoints

def convert_to_tflite(model, output_path, quantize=True, quantization_type='int8', representative_dataset=None):
    """
    Convertit un modÃ¨le Keras en TensorFlow Lite
    
    Args:
        model_path: Chemin vers le modÃ¨le SavedModel ou .h5
        output_path: Chemin de sortie pour le fichier .tflite
        quantize: Activer la quantization
        quantization_type: Type de quantization ('int8', 'float16', 'dynamic', 'none')
        representative_dataset: Dataset reprÃ©sentatif pour la quantization
    
    Returns:
        tflite_model_size: Taille du modÃ¨le en Ko
    """
    converter = tf.lite.TFLiteConverter.from_keras_model(model)
    
    # Configuration du converter selon le type de quantization
    if quantize:
        if quantization_type == 'int8':
            print("\nConfiguration de la quantization INT8 optimisÃ©e...")
            converter.optimizations = [tf.lite.Optimize.DEFAULT]
            converter.inference_input_type = tf.uint8
            converter.inference_output_type = tf.float32
            converter.target_spec.supported_ops = [
                tf.lite.OpsSet.TFLITE_BUILTINS_INT8,
                # tf.lite.OpsSet.TFLITE_BUILTINS,
            ]
            if representative_dataset is not None:
                converter.representative_dataset = representative_dataset
                
        elif quantization_type == 'float16':
            print("\nConfiguration de la quantization FLOAT16 (haute prÃ©cision)...")
            # converter.optimizations = [tf.lite.Optimize.DEFAULT]
            converter.target_spec.supported_types = [tf.float16]
            
        elif quantization_type == 'dynamic':
            print("\nConfiguration de la quantization dynamique (range-based)...")
            # converter.optimizations = [tf.lite.Optimize.DEFAULT]
            if representative_dataset is not None:
                converter.representative_dataset = representative_dataset
            # Les poids sont quantizÃ©s dynamiquement, entrÃ©es/sorties restent float32
            
        else:
            raise ValueError(f"Type de quantization non supportÃ©: {quantization_type}")
    else:
        print("\nâš™ï¸  Pas de quantization (modÃ¨le float32 complet)")
    
    # Convertir
    print("\nConversion en cours...")
    tflite_model = converter.convert()
    
    # Sauvegarder
    with open(output_path, 'wb') as f:
        f.write(tflite_model)
    
    # Afficher la taille
    tflite_model_size = len(tflite_model) / 1024  # en Ko
    
    return tflite_model_size


def create_representative_dataset_generator(X_val, num_samples=100):
    """
    CrÃ©e un gÃ©nÃ©rateur de dataset reprÃ©sentatif pour la quantization
    AMÃ‰LIORÃ‰: Utilise plus d'Ã©chantillons et couvre mieux la distribution
    
    Args:
        X_val: Dataset de validation
        num_samples: Nombre d'Ã©chantillons Ã  utiliser (augmentÃ© pour meilleure calibration)
    
    Returns:
        representative_dataset_gen: GÃ©nÃ©rateur pour le converter
    """
    def representative_dataset_gen():
        # AMÃ‰LIORATION 2: Utiliser TOUS les Ã©chantillons de validation pour meilleure calibration
        # Au lieu de prendre sÃ©quentiellement, on mÃ©lange pour couvrir toute la distribution
        indices = np.random.permutation(len(X_val))[:num_samples]
        for idx in indices:
            # Prendre un Ã©chantillon
            sample = X_val[idx:idx+1].astype(np.float32)
            yield [sample]
    
    return representative_dataset_gen


def test_tflite_model(tflite_path, val_ds,  num_samples=10):
    """
    Teste le modÃ¨le TFLite et compare avec les prÃ©dictions originales
    
    Args:
        tflite_path: Chemin vers le modÃ¨le .tflite
        X_test: Images de test
        y_test: Heatmaps de test
        num_samples: Nombre d'Ã©chantillons Ã  tester
    
    Returns:
        avg_error: Erreur moyenne
    """
    print("\nğŸ§ª Test du modÃ¨le TFLite...")
    
    # Charger l'interprÃ©teur TFLite
    tf_models = os.listdir(tflite_path)
    tf_models = [os.path.join(tflite_path, model) for model in tf_models if model.endswith(".tflite")]
    tf_models = [tf_models[1], tf_models[2], tf_models[0]]
    for tf_model in tf_models:
        try:
            model_type = tf_model.split("_")[-1].removesuffix(".tflite")
            interpreter = Interpreter(model_path=tf_model)
            interpreter.allocate_tensors()
            
            # Obtenir les dÃ©tails des entrÃ©es/sorties
            input_details = interpreter.get_input_details()
            output_details = interpreter.get_output_details()
            
            print(f"\nğŸ“Š DÃ©tails de l'interprÃ©teur:")
            print(f"   - Input shape: {input_details[0]['shape']}")
            print(f"   - Input type: {input_details[0]['dtype']}")
            print(f"   - Output shape: {output_details[0]['shape']}")
            print(f"   - Output type: {output_details[0]['dtype']}")
            
            # Tester sur quelques Ã©chantillons
            # get images and labels from test set
            all_images = []
            all_labels = []
            for images, labels in val_ds.unbatch().take(num_samples):
                # images = tf.cast(images, tf.uint8)
                images = np.array(images)[None, ...]
                labels = np.array(labels)
                # convert all colors of heatmaps to red
                # labels = np.stack([labels, labels, labels], axis=-1)
                all_images.append(images)
                all_labels.append(labels)

            tic = time.time()
            errors = None
            for i in range(min(num_samples, len(all_images))):
                # PrÃ©parer l'entrÃ©e
                input_data = all_images[i]
                input_data = tf.cast(input_data, tf.float32)
                
                # Si le modÃ¨le attend des uint8, il faut quantizer l'entrÃ©e
                if input_details[0]['dtype'] == np.uint8:
                    input_scale, input_zero_point = input_details[0]['quantization']
                    input_data = (input_data / input_scale + input_zero_point)
                    input_data = tf.cast(input_data, tf.uint8)
                
                # InfÃ©rence
                interpreter.set_tensor(input_details[0]['index'], input_data)
                interpreter.invoke()
                
                # RÃ©cupÃ©rer la sortie
                output_data = interpreter.get_tensor(output_details[0]['index'])
                
                # Si la sortie est quantizÃ©e, il faut la dÃ©quantizer
                if output_details[0]['dtype'] == np.uint8:
                    output_scale, output_zero_point = output_details[0]['quantization']
                    output_data = (output_data.astype(np.float32) - output_zero_point) * output_scale

                gt_coords, _ = from_heatmaps_to_coords(all_labels[i], from_prediction=False)
                pr_coords, _ = from_heatmaps_to_coords(output_data, from_prediction=True)
                rmse = np.sqrt(np.mean((gt_coords - pr_coords) ** 2, axis=1))[0]
                errors = np.vstack([errors, rmse]) if errors is not None else rmse
            print(" ----- Model: ", model_type)
            print("computed in: ", time.time() - tic)
            print("Error: ", np.mean(errors, axis=0), " pixel")
                # images_paths = os.path.join(model_dir, "images")
                # os.makedirs(os.path.join(model_dir, "images"), exist_ok=True)
                # import matplotlib.pyplot as plt
                # for i in range(len(gt_coords)):
                #     gt_coords_tmp = gt_coords[i]
                #     pr_coords_tmp = pr_coords[i]
                #     img_tmp = input_data[i]
                #     img_tmp = img_tmp.numpy().astype(np.uint8)
                #     [plt.scatter(gt_coords_tmp[1, i], gt_coords_tmp[0, i], color="r") for i in range(gt_coords_tmp.shape[-1])]
                #     [plt.scatter(pr_coords_tmp[1, i], pr_coords_tmp[0, i], color="b", s=10) for i in range(pr_coords_tmp.shape[-1])]
                #     plt.imshow(img_tmp)
                #     plt.show()
                    # plt.savefig(os.path.join(images_paths, f"image_{i}.png"))
                    # plt.close()    
        # return avg_error
        except Exception as e:
            print(f"Error occured while testing tflite model {model_type}: {e}")


def export_model(model, model_name="pose_model", model_dir=None, representative_ds=None):
    """
    Pipeline complet d'export du modÃ¨le en TFLite avec deux versions optimisÃ©es
    
    Args:
        model: ModÃ¨le Keras (optionnel si model_path est fourni)
        model_path: Chemin vers le modÃ¨le sauvegardÃ© (optionnel si model est fourni)
        X_val: Dataset de validation pour la quantization
        model_name: Nom du modÃ¨le
        model_dir: Dossier racine du modÃ¨le (si None, utilise config.MODELS_DIR)
    
    Returns:
        tflite_paths: Dictionnaire avec les chemins des modÃ¨les exportÃ©s
    """
    
    # DÃ©terminer le dossier des modÃ¨les
    models_dir = config.MODELS_DIR if model_dir is None else os.path.join(model_dir, "models")
    
    tflite_paths = {}
    
    # CrÃ©er le dataset reprÃ©sentatif si nÃ©cessaire
    representative_dataset = None
    if representative_ds is not None:
        def representative_dataset_gen():
            for images, _ in representative_ds.unbatch().take(100):
                images = tf.cast(images, tf.float32)
                yield [tf.expand_dims(images, 0)]

    print("\n" + "=" * 40)
    print("ğŸ“± EXPORT 1/2 - DYNAMIC RANGE QUANTIZATION")
    print("=" * 40)
    print("ğŸ¯ RECOMMANDÃ‰: PrÃ©cision optimale + taille rÃ©duite")
    
    tflite_dynamic_path = os.path.join(models_dir, f"{model_name}_dynamic.tflite")
    dynamic_size = convert_to_tflite(
        model=model,
        output_path=tflite_dynamic_path,
        quantize=True,
        quantization_type='dynamic',
        representative_dataset=None  # Dynamic n'a pas besoin de dataset reprÃ©sentatif
    )
    tflite_paths['dynamic'] = tflite_dynamic_path
    
    # Export 2: ModÃ¨le Float32 complet (haute prÃ©cision)
    print("\n" + "=" * 40)
    print("ğŸ”¬ EXPORT 2/2 - FLOAT32 COMPLET")
    print("=" * 40)
    print("ğŸ¯ TESTS: PrÃ©cision maximale (taille importante)")
    
    tflite_float32_path = os.path.join(models_dir, f"{model_name}_float32.tflite")
    float32_size = convert_to_tflite(
        model=model,
        output_path=tflite_float32_path,
        quantize=False,
        quantization_type='none',
        representative_dataset=None
    )
    tflite_paths['float32'] = tflite_float32_path

    # Export 3: ModÃ¨le int8 (smalest)
    print("\n" + "=" * 40)
    print("ğŸ”¬ EXPORT 2/2 - FLOAT32 COMPLET")
    print("=" * 40)
    print("ğŸ¯ TESTS: PrÃ©cision maximale (taille importante)")
    
    tflite_int8_path = os.path.join(models_dir, f"{model_name}_int8.tflite")
    int8_size = convert_to_tflite(
        model=model,
        output_path=tflite_int8_path,
        quantize=True,
        quantization_type='int8',
        representative_dataset=representative_dataset_gen
    )
    tflite_paths['int8'] = tflite_int8_path
    
    print(f"\nâœ… Exports terminÃ©s!")
    print(f"ğŸ“± ModÃ¨le Dynamic: {tflite_dynamic_path} ({dynamic_size:.1f} Ko)")
    print(f"ğŸ”¬ ModÃ¨le Float32: {tflite_float32_path} ({float32_size:.1f} Ko)")
    
    # Comparaison des modÃ¨les
    print("\n" + "=" * 60)
    print("ï¿½ COMPARAISON DES MODÃˆLES EXPORTÃ‰S")
    print("=" * 60)
    print("ModÃ¨le         | Taille | PrÃ©cision | Usage recommandÃ©")
    print("-" * 60)
    print(f"Dynamic (.tflite) | {dynamic_size:>5.1f} Ko | ~1px erreur | PRODUCTION MOBILE â­")
    print(f"Float32 (.tflite) | {float32_size:>5.1f} Ko | ~0px erreur | TESTS/VALIDATION")
    print("=" * 60)
    
    # Instructions pour l'utilisation
    print("\nğŸ“± UTILISATION DANS FLUTTER")
    print("=" * 60)
    print("ğŸ¤– Pour production mobile:")
    print(f"   ğŸ“ Utilisez: {os.path.basename(tflite_dynamic_path)}")
    print("   âœ… PrÃ©cision suffisante + taille optimisÃ©e")
    print("   ğŸš€ Compatible avec GPU/NNAPI delegates")
    
    print("\nğŸ”¬ Pour tests/validation:")
    print(f"   ğŸ“ Utilisez: {os.path.basename(tflite_float32_path)}")
    print("   âœ… PrÃ©cision maximale")
    print("   ğŸŒ Plus lent, taille importante")
    
    print("\nğŸ“‹ ParamÃ¨tres communs:")
    print("   â€¢ Input: 192Ã—192Ã—3 float32 (0-1 normalisÃ©)")
    print("   â€¢ Output: 48Ã—48Ã—3 float32 (heatmaps)")
    print("   â€¢ Keypoints: [0]=Hanche, [1]=Genou, [2]=Cheville")
    print("=" * 60)
    
    return tflite_paths


if __name__ == "__main__":
    print("âœ… Module export_tflite.py chargÃ© avec succÃ¨s")
    print("ğŸ“ Utilisez main.py pour exporter le modÃ¨le aprÃ¨s l'entraÃ®nement")
