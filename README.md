# Fine-tuning Pose Estimation

Mod√®le de pose estimation fine-tun√© avec support multi-backbones pour d√©tecter 3 keypoints : hanche, genou, cheville.

**Backbones support√©s** : MobileNetV2 (d√©faut), MobileNetV3, EfficientNetLite0-4, EfficientNetB0-3, EfficientNetV2B0-3

## Installation

### Avec Conda (recommand√©)

```bash
# Cloner/installer l'environnement
./install_conda.sh

# Activer l'environnement
conda activate pose-estimation
```

### Avec pip

```bash
pip install -r requirements.txt
```

## Utilisation

### Pipeline complet (entra√Ænement + export)

```bash
# Avec MobileNetV2 (d√©faut - rapide et l√©ger)
python main.py --save-data

# Avec EfficientNetLite (meilleure pr√©cision, optimis√© mobile)
python main.py --save-data --backbone EfficientNetLite0

# Avec EfficientNetV2 (haute pr√©cision)
python main.py --save-data --backbone EfficientNetV2B0
```

### Utiliser un mod√®le d√©j√† entra√Æn√©

```bash
# Charger depuis un mod√®le sp√©cifique
python main.py --skip-data-prep --skip-training --model-path output/models/pose_model_YYYYMMDD_HHMMSS_saved_model
```

## Test du mod√®le

### Sur une vid√©o (TFLite - recommand√© pour production)

```bash
python test_video.py --video "votre_video.mp4"
# Sortie: votre_video_dynamic_annotated.mp4
```

### Sur une vid√©o (TFLite haute pr√©cision - pour validation)

```bash
python test_video.py --video "votre_video.mp4" --model "output/models/pose_model_float32.tflite"
# Sortie: votre_video_float32_annotated.mp4
```

### Sur une vid√©o (Keras - pour validation)

```bash
python test_video_keras.py --video "votre_video.mp4"
# Sortie: votre_video_keras_annotated.mp4
```

### Sur une vid√©o (Keras - pour validation)

```bash
python test_video_keras.py --video "votre_video.mp4"
```

### Comparer pr√©cision Keras vs TFLite

```bash
python quick_compare.py
# Compare Keras vs TFLite Dynamic (mod√®le recommand√©)
# G√©n√®re: *_keras_annotated.mp4 et *_dynamic_annotated.mp4
```

### Export analyse vid√©o en CSV (comparaison mod√®les)

```bash
# Exporte les positions des keypoints pour tous les formats de mod√®les
python export_video_analysis.py --video "votre_video.mp4" --model-dir "output/MNv2_20251113_123456"

# Sortie:
#   - votre_video_analysis.csv (format long)
#   - votre_video_analysis_pivot.csv (format pivot pour comparaison)
# Compare: Keras, TFLite float32, dynamic, int8
```

# G√©n√®re: _\_keras_annotated.mp4 et _\_dynamic_annotated.mp4

````

### Pr√©diction sur une image

```bash
python predict.py --image "votre_image.jpg" --model "output/models/pose_model_best.h5"
````

## Options principales

### main.py

- `--backbone` : Choix du backbone (MobileNetV2, EfficientNetLite0-4, etc. - d√©faut: MobileNetV2)
- `--skip-data-prep` : Utiliser les donn√©es pr√©trait√©es
- `--skip-training` : Charger un mod√®le existant
- `--skip-export` : Ne pas exporter en TFLite
- `--save-data` : Sauvegarder les donn√©es pr√©trait√©es
- `--model-path` : Chemin vers un mod√®le existant

### test_video.py / test_video_keras.py

- `--video` : Chemin vers la vid√©o √† analyser
- `--model` : Chemin vers le mod√®le (optionnel)

## Donn√©es d'entra√Ænement

Les donn√©es doivent √™tre organis√©es comme suit :

```
labeled-data/
‚îú‚îÄ‚îÄ 101D/
‚îÇ   ‚îú‚îÄ‚îÄ CollectedData_*.csv    # Fichier CSV DeepLabCut (nom variable)
‚îÇ   ‚îî‚îÄ‚îÄ [images .png]
‚îú‚îÄ‚îÄ 101D_labeled/              # Dossier ignor√© automatiquement
‚îî‚îÄ‚îÄ ...
```

Format CSV DeepLabCut avec colonnes :

- Colonne 2 : nom de l'image
- Colonnes 3-4 : hanche (x,y)
- Colonnes 5-6 : genou (x,y)
- Colonnes 7-8 : cheville (x,y)

## R√©sultats

Apr√®s ex√©cution, les fichiers sont sauvegard√©s dans `output/` avec une structure organis√©e :

```
output/
‚îî‚îÄ‚îÄ Backbone_Date/                    # ex: MNv2_20251108_190128/
    ‚îú‚îÄ‚îÄ models/                       # Mod√®les entra√Æn√©s
    ‚îÇ   ‚îú‚îÄ‚îÄ pose_model_best.h5        # Meilleur mod√®le Keras
    ‚îÇ   ‚îú‚îÄ‚îÄ pose_model_final.h5       # Mod√®le final Keras
    ‚îÇ   ‚îú‚îÄ‚îÄ pose_model_saved_model/   # SavedModel pour TFLite
    ‚îÇ   ‚îú‚îÄ‚îÄ pose_model_dynamic.tflite
    ‚îÇ   ‚îî‚îÄ‚îÄ pose_model_float32.tflite
    ‚îú‚îÄ‚îÄ logs/                         # Logs et m√©triques
    ‚îÇ   ‚îú‚îÄ‚îÄ pose_model_YYYYMMDD-HHMMSS/  # TensorBoard
    ‚îÇ   ‚îú‚îÄ‚îÄ pose_model_history.png    # Courbes d'apprentissage
    ‚îÇ   ‚îî‚îÄ‚îÄ pose_model_training_log.csv # Logs CSV
    ‚îú‚îÄ‚îÄ videos/                       # Vid√©os annot√©es de test
    ‚îî‚îÄ‚îÄ preprocessed_data.npz         # Donn√©es pr√©trait√©es
```

### Mod√®les export√©s

- **Dynamic (.tflite)** ‚≠ê RECOMMAND√â : 6MB, pr√©cision ~1px, production mobile
- **Float32 (.tflite)** üî¨ TESTS : 22MB, pr√©cision maximale, validation

## M√©triques

Le mod√®le atteint g√©n√©ralement (r√©sultats du dernier test) :

- **Pr√©cision finale** : MAE = 0.119 (pixels)
- **Taille mod√®le Dynamic** : ~6MB (optimis√© pour mobile)
- **Taille mod√®le Float32** : ~22MB (haute pr√©cision)
- **Vitesse** : ~30 FPS sur CPU mobile
- **Convergence** : Loss de 0.163 ‚Üí 0.015 en 5 epochs

## Architecture

- **Backbone** : Multi-backbone support (MobileNetV2 par d√©faut, EfficientNetLite, EfficientNetB, EfficientNetV2)
- **T√™te** : D√©convolution 3 √©tages avec adaptation automatique √† la sortie du backbone
- **Sortie** : Heatmaps 48x48x3
- **Fine-tuning** : Backbone gel√©, seulement la t√™te entra√Æn√©e
- **Augmentation** : Rotation, translation, zoom, flip horizontal

### Backbones disponibles

**L√©gers (mobile/edge) :**

- `MobileNetV2` (‚≠ê d√©faut) : 192x192, ~3.5M params, tr√®s rapide
- `MobileNetV3Small` : 192x192, ~2.5M params, ultra-l√©ger
- `EfficientNetLite0-4` : 224-300px, pr√©cision progressive

**Haute pr√©cision :**

- `EfficientNetB0-3` : 224-300px, meilleure pr√©cision
- `EfficientNetV2B0-3` : 224-300px, entra√Ænement plus rapide
