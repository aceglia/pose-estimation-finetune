# Fine-tuning Pose Estimation

ModÃ¨le de pose estimation fine-tunÃ© sur MobileNetV2 pour dÃ©tecter 3 keypoints : hanche, genou, cheville.

## Installation

### Avec Conda (recommandÃ©)

```bash
# Cloner/installer l'environnement
./install_conda.sh

# Activer l'environnement
conda activate pose-estimation
```

### Avec pip

```bash
pip install -r requirements.txt
```

## Utilisation

### Pipeline complet (entraÃ®nement + export)

```bash
python main.py --save-data
```

### Utiliser un modÃ¨le dÃ©jÃ  entraÃ®nÃ©

```bash
# Charger depuis un modÃ¨le spÃ©cifique
python main.py --skip-data-prep --skip-training --model-path output/models/pose_model_YYYYMMDD_HHMMSS_saved_model
```

## Test du modÃ¨le

### Sur une vidÃ©o (TFLite - recommandÃ© pour production)

```bash
python test_video.py --video "votre_video.mp4"
# Sortie: votre_video_dynamic_annotated.mp4
```

### Sur une vidÃ©o (TFLite haute prÃ©cision - pour validation)

```bash
python test_video.py --video "votre_video.mp4" --model "output/models/pose_model_float32.tflite"
# Sortie: votre_video_float32_annotated.mp4
```

### Sur une vidÃ©o (Keras - pour validation)

```bash
python test_video_keras.py --video "votre_video.mp4"
# Sortie: votre_video_keras_annotated.mp4
```

### Sur une vidÃ©o (Keras - pour validation)

```bash
python test_video_keras.py --video "votre_video.mp4"
```

### Comparer prÃ©cision Keras vs TFLite

```bash
python quick_compare.py
# Compare Keras vs TFLite Dynamic (modÃ¨le recommandÃ©)
# GÃ©nÃ¨re: *_keras_annotated.mp4 et *_dynamic_annotated.mp4
```

### PrÃ©diction sur une image

```bash
python predict.py --image "votre_image.jpg" --model "output/models/pose_model_best.h5"
```

## Options principales

### main.py

- `--skip-data-prep` : Utiliser les donnÃ©es prÃ©traitÃ©es
- `--skip-training` : Charger un modÃ¨le existant
- `--skip-export` : Ne pas exporter en TFLite
- `--save-data` : Sauvegarder les donnÃ©es prÃ©traitÃ©es
- `--model-path` : Chemin vers un modÃ¨le existant

### test_video.py / test_video_keras.py

- `--video` : Chemin vers la vidÃ©o Ã  analyser
- `--model` : Chemin vers le modÃ¨le (optionnel)

## DonnÃ©es d'entraÃ®nement

Les donnÃ©es doivent Ãªtre organisÃ©es comme suit :

```
labeled-data/
â”œâ”€â”€ 101D/
â”‚   â”œâ”€â”€ CollectedData_*.csv    # Fichier CSV DeepLabCut (nom variable)
â”‚   â””â”€â”€ [images .png]
â”œâ”€â”€ 101D_labeled/              # Dossier ignorÃ© automatiquement
â””â”€â”€ ...
```

Format CSV DeepLabCut avec colonnes :

- Colonne 2 : nom de l'image
- Colonnes 3-4 : hanche (x,y)
- Colonnes 5-6 : genou (x,y)
- Colonnes 7-8 : cheville (x,y)

## RÃ©sultats

AprÃ¨s exÃ©cution, les fichiers sont sauvegardÃ©s dans `output/` avec une structure organisÃ©e :

```
output/
â””â”€â”€ Backbone_Date/                    # ex: MNv2_20251108_190128/
    â”œâ”€â”€ models/                       # ModÃ¨les entraÃ®nÃ©s
    â”‚   â”œâ”€â”€ pose_model_best.h5        # Meilleur modÃ¨le Keras
    â”‚   â”œâ”€â”€ pose_model_final.h5       # ModÃ¨le final Keras
    â”‚   â”œâ”€â”€ pose_model_saved_model/   # SavedModel pour TFLite
    â”‚   â”œâ”€â”€ pose_model_dynamic.tflite
    â”‚   â””â”€â”€ pose_model_float32.tflite
    â”œâ”€â”€ logs/                         # Logs et mÃ©triques
    â”‚   â”œâ”€â”€ pose_model_YYYYMMDD-HHMMSS/  # TensorBoard
    â”‚   â”œâ”€â”€ pose_model_history.png    # Courbes d'apprentissage
    â”‚   â””â”€â”€ pose_model_training_log.csv # Logs CSV
    â”œâ”€â”€ videos/                       # VidÃ©os annotÃ©es de test
    â””â”€â”€ preprocessed_data.npz         # DonnÃ©es prÃ©traitÃ©es
```

### ModÃ¨les exportÃ©s

- **Dynamic (.tflite)** â­ RECOMMANDÃ‰ : 6MB, prÃ©cision ~1px, production mobile
- **Float32 (.tflite)** ğŸ”¬ TESTS : 22MB, prÃ©cision maximale, validation

## MÃ©triques

Le modÃ¨le atteint gÃ©nÃ©ralement (rÃ©sultats du dernier test) :

- **PrÃ©cision finale** : MAE = 0.119 (pixels)
- **Taille modÃ¨le Dynamic** : ~6MB (optimisÃ© pour mobile)
- **Taille modÃ¨le Float32** : ~22MB (haute prÃ©cision)
- **Vitesse** : ~30 FPS sur CPU mobile
- **Convergence** : Loss de 0.163 â†’ 0.015 en 5 epochs

## Architecture

- **Backbone** : MobileNetV2 (prÃ©-entraÃ®nÃ© sur ImageNet)
- **TÃªte** : DÃ©convolution 3 Ã©tages
- **Sortie** : Heatmaps 48x48x3
- **Fine-tuning** : Backbone gelÃ©, seulement la tÃªte entraÃ®nÃ©e
- **Augmentation** : Rotation, translation, zoom, flip horizontal
